{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"vqa_model.ipynb","provenance":[],"collapsed_sections":["ywPKRVxIGTIq","ZdHFSo1eo2Wa","CVBYOc5lcj_L"],"machine_shape":"hm","mount_file_id":"1VipQedHAPxrPlVHWQU2yJIbZ2WfTVVMK","authorship_tag":"ABX9TyNjke5SarCnKUr8lLWrmbZ0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"YJEi-ho_Yfxh"},"source":["# !pip install git+https://github.com/SKT-AI/KoBART#egg=kobart\n","!pip3 install kobert-transformers\n","!pip install customized_konlpy\n","!pip install sentencepiece"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I1kTyammN3xA"},"source":["from tensorflow.keras.layers import Input, Dense, concatenate, Dropout, Flatten, Embedding, LayerNormalization, Reshape, Conv2D, MaxPooling2D\n","from tensorflow.keras.models import Model\n","from keras.preprocessing import image, sequence\n","from keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.utils import to_categorical, Sequence\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import keras\n","from PIL import Image\n","from kobert_transformers import get_tokenizer\n","from ckonlpy.tag import Twitter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qGIbsUKb-Nki"},"source":["!unzip \"./drive/My Drive/vqa/train.zip\" -d \"./\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v8ZGA-pIPjZ9"},"source":["###하이퍼파라미터"]},{"cell_type":"code","metadata":{"id":"UORZMh_TPiZL"},"source":["VOCAB_SIZE = 10000\n","d_model = 256\n","max_len = 40\n","dff = 512\n","num_heads = 16\n","num_layers = 5\n","dropout = 0.1\n","EPOCHS = 10\n","BATCH_SIZE = 100\n","TARGET_SIZE = 224"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ywPKRVxIGTIq"},"source":["###콜백\n"]},{"cell_type":"code","metadata":{"id":"IrQQ-WDpGVyQ"},"source":["callback_list = [\n","  keras.callbacks.EarlyStopping(\n","    monitor='val_accuracy', # 모델의 검증 정확도 모니터링\n","    patience=10, # 10 에포크보다 더 길게 향상되지 않으면 중단\n","  ),\n","  keras.callbacks.ModelCheckpoint(\n","    filepath='my_model.h5', # 저장\n","    monitor='val_loss',\n","    save_best_only=True, # 가장 좋은 모델\n","    save_weights_only=True\n","  )\n","]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZdHFSo1eo2Wa"},"source":["### 기본 함수"]},{"cell_type":"code","metadata":{"id":"fjC7vMNqg-yo"},"source":["class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n","    def __init__(self, max_len, vocab_size, embedding_dim):\n","        super(TokenAndPositionEmbedding, self).__init__()\n","        self.token_emb = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.pos_emb = tf.keras.layers.Embedding(max_len, embedding_dim)\n","        self.vocab_size = vocab_size\n","        self.max_len = max_len\n","        self.embedding_dim = embedding_dim\n","    \n","    def get_config(self):\n","\n","        config = super().get_config().copy()\n","        config.update({\n","            'vocab_size': self.vocab_size,\n","            'max_len': self.max_len,\n","            'embedding_dim': self.embedding_dim,\n","            'token_emb': self.token_emb,\n","            'pos_emb': self.pos_emb\n","        })\n","        return config\n","\n","    def call(self, x):\n","        max_len = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=max_len, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x + positions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l7CGwtFMinl7"},"source":["# 값으로 들어온 query, key, value를 통해 어텐션 메커니즘 수행\n","# scaled dot product 계산 \n","# 멀티헤드 어텐션을 구현하므로, d_model/num_heads의 크기를 가진다.\n","# 문장의 길이는 그 전에 제로 패딩을 통해 전부 맞춰준다.\n","# num_heads는 멀티 헤드의 개수\n","# batch_size는 훈련을 시킬 때 사용하는 배치사이즈\n","# 출력 값\n","# output : softmax 함수에 value 행렬을 곱해준 벡터\n","# attention_weights : softmax 함수까지 연산된 행렬 (어텐션 스코어 행렬?)\n","\n","def scaled_dot_product_attention(query, key, value, mask):\n","  # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n","  # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n","  # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n","  # padding_mask : (batch_size, 1, 1, key의 문장 길이)\n","\n","  # Q와 K의 곱. 어텐션 스코어 행렬.\n","  matmul_qk = tf.matmul(query, key, transpose_b=True)\n","\n","  # 스케일링\n","  # dk의 루트값으로 나눠준다.\n","  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n","  logits = matmul_qk / tf.math.sqrt(depth)\n","\n","  # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.\n","  # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.\n","  if mask is not None:\n","    logits += (mask * -1e9)\n","\n","  # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.\n","  # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)\n","  attention_weights = tf.nn.softmax(logits, axis=-1)\n","\n","  # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n","  output = tf.matmul(attention_weights, value)\n","\n","  return output, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DHUKfMz9ipA-"},"source":["# 멀티헤드 어텐션을 만드는 클래스\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","\n","  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n","    super(MultiHeadAttention, self).__init__(name=name)\n","    self.num_heads = num_heads\n","    self.d_model = d_model\n","\n","    assert d_model % self.num_heads == 0\n","\n","    # d_model을 num_heads로 나눈 값.\n","    # 논문 기준 : 64\n","    self.depth = d_model // self.num_heads\n","\n","    # WQ, WK, WV에 해당하는 밀집층 정의\n","    self.query_dense = tf.keras.layers.Dense(units=d_model,kernel_regularizer=keras.regularizers.l2(0.001))\n","    self.key_dense = tf.keras.layers.Dense(units=d_model,kernel_regularizer=keras.regularizers.l2(0.001))\n","    self.value_dense = tf.keras.layers.Dense(units=d_model,kernel_regularizer=keras.regularizers.l2(0.001))\n","\n","    # WO에 해당하는 밀집층 정의\n","    self.dense = tf.keras.layers.Dense(units=d_model,kernel_regularizer=keras.regularizers.l2(0.001))\n","\n","  def get_config(self):\n","    config = super().get_config().copy()\n","    config.update({\n","        'd_model': self.d_model,\n","        'num_heads': self.num_heads,\n","        'depth': self.depth,\n","        'query_dense': self.query_dense,\n","        'key_dense' : self.key_dense,\n","        'value_dense': self.value_dense,\n","        'dense': self.dense\n","    })\n","    return config\n","\n","  # num_heads 개수만큼 q, k, v를 split하는 함수\n","  def split_heads(self, inputs, batch_size):\n","    inputs = tf.reshape(\n","        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n","    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n","\n","  def call(self, inputs):\n","    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n","        'value'], inputs['mask']\n","    batch_size = tf.shape(query)[0]\n","\n","    # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n","    # q : (batch_size, query의 문장 길이, d_model)\n","    # k : (batch_size, key의 문장 길이, d_model)\n","    # v : (batch_size, value의 문장 길이, d_model)\n","    # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n","    query = self.query_dense(query)\n","    key = self.key_dense(key)\n","    value = self.value_dense(value)\n","\n","    # 2. 헤드 나누기\n","    # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n","    # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n","    # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n","    query = self.split_heads(query, batch_size)\n","    key = self.split_heads(key, batch_size)\n","    value = self.split_heads(value, batch_size)\n","\n","    # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n","    # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n","    scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n","    # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","\n","    # 4. 헤드 연결(concatenate)하기\n","    # (batch_size, query의 문장 길이, d_model)\n","    concat_attention = tf.reshape(scaled_attention,\n","                                  (batch_size, -1, self.d_model))\n","\n","    # 5. WO에 해당하는 밀집층 지나기\n","    # (batch_size, query의 문장 길이, d_model)\n","    outputs = self.dense(concat_attention)\n","\n","    return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aWzQ7UaUitu6"},"source":["# 아래 멀티-헤드 어텐션 부분에서 query, key, value가 전부 같으므로 \n","# 셀프 어텐션을 이용하게 되는 것.\n","# 셀프 어텐션이 아닐 경우에는, 해당 부분을 수정해주면 된다.\n","\n","def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\n","  inputs = Input(shape=(max_len, d_model), name=\"inputs_layer\")\n","\n","  # 인코더는 패딩 마스크 사용\n","  padding_mask = Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","  # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)\n","  attention = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention\")({\n","          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n","          'mask': padding_mask # 패딩 마스크 사용\n","      })\n","\n","  # 드롭아웃 + 잔차 연결과 층 정규화\n","  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n","  attention = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(inputs + attention)\n","\n","  # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)\n","  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\n","  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","\n","  # 드롭아웃 + 잔차 연결과 층 정규화\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","  outputs = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention + outputs)\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, padding_mask], outputs=outputs, name=name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SoutVfBni2UC"},"source":["# 아래는 셀프-어텐션 용 인코더\n","# 질문 분석에만 쓰이고, 이미지와 결합시킬 때는 약간 수정을 해주자.\n","\n","def encoder(vocab_size, num_layers, dff,\n","            d_model, num_heads, dropout,\n","            name=\"encoder\"):\n","  inputs = Input(shape=(max_len, d_model), name=\"inputs_enc\")\n","\n","  # 인코더는 패딩 마스크 사용\n","  padding_mask = Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","  outputs = Dropout(rate=dropout)(inputs)\n","\n","  # 인코더를 num_layers개 쌓기\n","  for i in range(num_layers):\n","    outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n","        dropout=dropout, name=\"encoder_layer_{}\".format(i),\n","    )([outputs, padding_mask])\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, padding_mask], outputs=outputs, name=name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Lv1grPyi5Wv"},"source":["def loss_function(y_true, y_pred):\n","  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","\n","  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","      from_logits=True, reduction='none')(y_true, y_pred)\n","\n","  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n","  loss = tf.multiply(loss, mask)\n","\n","  return tf.reduce_mean(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AJ4hknPAjC-u"},"source":["# 학습률을 조정해주는 함수\n","class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","\n","  def __init__(self, d_model, warmup_steps=4000):\n","    super(CustomSchedule, self).__init__()\n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","    self.warmup_steps = warmup_steps\n","\n","  def get_config(self):\n","    config = {\n","    'd_model': self.d_model,\n","    'warmup_steps': self.warmup_steps,\n","\n","     }\n","    return config\n","\n","  def __call__(self, step):\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps**-1.5)\n","\n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xw4Zy6h9kygF"},"source":["# txt 가 쿼리\n","# img 가 key + value\n","\n","def txtimg_encoder_layer(dff, d_model, num_heads, dropout, name='imgtxt_encoder_layer'):\n","  txtInput = Input(shape=(max_len, d_model), name=\"txtInputs_Query\")\n","  imgInput = Input(shape=(max_len, d_model), name=\"imgInputs_Key_Value\")\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","  # 멀티-헤드 어텐션 (Query : Txt, Key = Value : Image)\n","  imgAttention = MultiHeadAttention(\n","    d_model, num_heads, name=\"img_txt_attention\")(inputs={\n","        'query': txtInput, 'key': imgInput, 'value': imgInput, # Q != K = V\n","        'mask': padding_mask # 패딩 마스크\n","    })\n","\n","  # 잔차 + 정규화\n","  imgAttention = Dropout(rate=dropout)(imgAttention)\n","  imgAttention = LayerNormalization(epsilon=1e-6)(imgAttention + txtInput)\n","\n","  # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)\n","  outputs = Dense(units=dff, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001))(imgAttention)\n","  outputs = Dense(units=d_model)(outputs)\n","\n","  # 드롭아웃 + 잔차 연결과 층 정규화\n","  outputs = Dropout(rate=dropout)(outputs)\n","  outputs = LayerNormalization(epsilon=1e-6)(outputs + imgAttention)\n","\n","  return tf.keras.Model(inputs=[txtInput, imgInput, padding_mask],\n","      outputs=outputs,\n","      name=name)\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B2YKkTE4t1XD"},"source":["# txt가 쿼리\n","# img가 키와 value\n","\n","def txtimg_encoder(vocab_size, num_layers, dff,\n","            d_model, num_heads, dropout,\n","            name=\"txtimg_encoder\"):\n","  txtInput = Input(shape=(max_len, d_model, ), name=\"txt inputs\")\n","  imgInput = Input(shape=(max_len, d_model, ), name=\"img inputs\")\n","\n","  # 인코더는 패딩 마스크 사용\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(txtInput)\n","\n","  # 인코더를 num_layers개 쌓기\n","  for i in range(num_layers):\n","    outputs = txtimg_encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n","        dropout=dropout, name=\"txtimg_encoder_layer_{}\".format(i),\n","    )([outputs, imgInput ,padding_mask])\n","\n","  return tf.keras.Model(\n","      inputs=[txtInput, imgInput, padding_mask], outputs=outputs, name=name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FEh7xMKUqdGV"},"source":["# img 가 쿼리\n","# txt 가 key + value\n","\n","def imgtxt_encoder_layer(dff, d_model, num_heads, dropout, name='imgtxt_encoder_layer'):\n","  imgInput = Input(shape=(max_len, d_model), name=\"imgInputs_Query\")\n","  txtInput = Input(shape=(max_len, d_model), name=\"txtInputs_KeyValue\")\n","\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","  # 멀티-헤드 어텐션 (Query : Txt, Key = Value : Image)\n","  txtAttention = MultiHeadAttention(\n","    d_model, num_heads, name=\"img_txt_attention\")(inputs={\n","        'query': imgInput, 'key': txtInput, 'value': txtInput, # Q != K = V\n","        'mask': padding_mask # 패딩 마스크\n","    })\n","\n","  # 잔차 + 정규화\n","  txtAttention = Dropout(rate=dropout)(txtAttention)\n","  txtAttention = LayerNormalization(epsilon=1e-6)(txtAttention + imgInput)\n","\n","  # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)\n","  outputs = Dense(units=dff, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001))(txtAttention)\n","  outputs = Dense(units=d_model)(outputs)\n","\n","  # 드롭아웃 + 잔차 연결과 층 정규화\n","  outputs = Dropout(rate=dropout)(outputs)\n","  outputs = LayerNormalization(epsilon=1e-6)(outputs + txtAttention)\n","\n","  return tf.keras.Model(inputs=[imgInput, txtInput, padding_mask],\n","      outputs=outputs,\n","      name=name)\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OWwLBl-juKY3"},"source":["# img가 쿼리\n","# txt가 키와 value\n","\n","def imgtxt_encoder(vocab_size, num_layers, dff,\n","            d_model, num_heads, dropout,\n","            name=\"imgtxt_encoder\"):\n","  imgInput = Input(shape=(max_len, d_model,), name=\"img inputs\")\n","  txtInput = Input(shape=(max_len, d_model,), name=\"txt inputs\")\n","  # 인코더는 패딩 마스크 사용\n","  padding_mask = Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","  outputs = Dropout(rate=dropout)(imgInput)\n","\n","  # 인코더를 num_layers개 쌓기\n","  for i in range(num_layers):\n","    outputs = imgtxt_encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n","        dropout=dropout, name=\"imgtxt_encoder_layer_{}\".format(i),\n","    )([outputs, txtInput ,padding_mask])\n","\n","  return Model(\n","      inputs=[imgInput, txtInput, padding_mask], outputs=outputs, name=name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1YdcmmM5-cXD"},"source":["def create_padding_mask(x):\n","  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n","  # (batch_size, 1, 1, key의 문장 길이)\n","  return mask[:, tf.newaxis, tf.newaxis, :]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zKzMos59jVsa"},"source":["# X는 train_data (image, question, answer)\n","# batch_size : BATCH_SIZE\n","# target_size : 이미지 크기\n","# shuffle : 셔플\n","\n","class DataGenerator(Sequence):\n","  def __init__(self, X, tokenizer, vocab_size, batch_size, target_size = (224,224), shuffle = True):\n","    self.X = X\n","    self.length = len(X)\n","    self.batch_size = batch_size\n","    self.target_size = target_size\n","    self.shuffle = shuffle\n","    self.tokenizer = tokenizer\n","    self.vocab_size = vocab_size\n","    self.on_epoch_end()\n","  \n","  def on_epoch_end(self):\n","    self.indexes = np.arange(self.length)\n","    if self.shuffle:\n","      np.random.shuffle(self.indexes)\n","\n","  def __len__(self):\n","    return int(np.floor(self.length / self.batch_size))\n","\n","  def __data_generation(self, i):  \n","    filepath = './images/' + self.X.iloc[i]['image']\n","    img = Image.open(filepath)\n","    img.draft('RGB', (224,224))\n","    img = img.resize(self.target_size, Image.NEAREST)\n","    img = np.array(img)\n","    img = img / 255.0\n","\n","    tokenized = self.tokenizer.tokenize(self.X.iloc[i]['question'])\n","    question = self.tokenizer.convert_tokens_to_ids(tokenized)\n","\n","    self.tokenizer.add_tokens(self.X.iloc[i]['answer'])\n","    answer = self.tokenizer.convert_tokens_to_ids(self.X.iloc[i]['answer'])\n","\n","    return img, question, answer\n","\n","  def __getitem__(self, index):\n","    img_data = np.empty((0,224,224,3))\n","    quest_data = []\n","    ans_data = []\n","    indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n","\n","    for i in indexes:\n","      img, question, answer = self.__data_generation(i)\n","      img_data = np.append(img_data, [img], axis=0)\n","      quest_data.append(question)\n","      ans_data.append(answer)\n","\n","    quest_data = sequence.pad_sequences(quest_data, maxlen=40, padding='post')\n","    ans_data = to_categorical(ans_data, num_classes = self.vocab_size)\n","\n","    return [quest_data, img_data], ans_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WapiPi8ciWJJ"},"source":["# X는 train_data (image, question, answer)\n","# batch_size : BATCH_SIZE\n","# target_size : 이미지 크기\n","# shuffle : 셔플\n","\n","class DataGenerator_test(Sequence):\n","  def __init__(self, X, tokenizer, vocab_size, batch_size, target_size = (224,224), shuffle = True):\n","    self.X = X\n","    self.length = len(X)\n","    self.batch_size = batch_size\n","    self.target_size = target_size\n","    self.shuffle = shuffle\n","    self.tokenizer = tokenizer\n","    self.vocab_size = vocab_size\n","    self.on_epoch_end()\n","  \n","  def on_epoch_end(self):\n","    self.indexes = np.arange(self.length)\n","    if self.shuffle:\n","      np.random.shuffle(self.indexes)\n","\n","  def __len__(self):\n","    return int(np.floor(self.length / self.batch_size))\n","\n","  def __data_generation(self, i):  \n","    filepath = './test/images/' + self.X.iloc[i]['image']\n","    img = Image.open(filepath)\n","    img.draft('RGB', (224,224))\n","    img = img.resize(self.target_size, Image.NEAREST)\n","    img = np.array(img)\n","    img = img / 255.0\n","\n","    tokenized = self.tokenizer.tokenize(self.X.iloc[i]['question'])\n","    question = self.tokenizer.convert_tokens_to_ids(tokenized)\n","\n","    return img, question\n","\n","  def __getitem__(self, index):\n","    img_data = np.empty((0,224,224,3))\n","    quest_data = []\n","    indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n","\n","    for i in indexes:\n","      img, question = self.__data_generation(i)\n","      img_data = np.append(img_data, [img], axis=0)\n","      quest_data.append(question)\n","\n","    quest_data = sequence.pad_sequences(quest_data, maxlen=40, padding='post')\n","\n","    return [quest_data, img_data]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CVBYOc5lcj_L"},"source":["###데이터 제너레이터"]},{"cell_type":"code","metadata":{"id":"Gv5v0vXIRrqa"},"source":["train_data = pd.read_csv('./drive/MyDrive/vqa/train.csv')\n","train_data.drop(['category'], axis=1,inplace=True)\n","\n","tokenizer = get_tokenizer()\n","\n","train_data, valid_data, train_y, valid_y = train_test_split(train_data, train_data['answer'], test_size=0.01, random_state=77)\n","\n","train_generator = DataGenerator(train_data, tokenizer=tokenizer, vocab_size=VOCAB_SIZE, \n","                  batch_size=BATCH_SIZE,\n","                  target_size = (TARGET_SIZE, TARGET_SIZE),\n","                  shuffle=True)\n","\n","valid_generator = DataGenerator(valid_data, tokenizer=tokenizer, vocab_size=VOCAB_SIZE, \n","                  batch_size=BATCH_SIZE,\n","                  target_size = (TARGET_SIZE, TARGET_SIZE),\n","                  shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gK0wd-rko8KY"},"source":["###모델링\n"]},{"cell_type":"code","metadata":{"id":"T6sIjmMKPWUr"},"source":["# 여기서 문장의 길이를 max_len으로 맞춰주자.\n","txtInput = Input(shape=(max_len, ), name='text_input')\n","imgInput = Input(shape=(224, 224, 3, ), name='img_input')\n","\n","padding_mask = tf.keras.layers.Lambda(\n","      create_padding_mask, output_shape=(1, 1, None),\n","      name='enc_padding_mask')(txtInput)\n","\n","# 텍스트 입력 후 Transformer Encoder 통과\n","# 형태소가 나누어져서 각 단어마다 정수 인코딩이 된 상태라고 가정.(토큰화 완료)\n","# 사전 훈련된 한국어 임베딩 모델....있나?\n","# tokenizer은 koBERT의 토크나이저 사용\n","# 임베딩은 pre-trained를 사용하지 않고 질문에서 학습\n","\n","embedding_layer = TokenAndPositionEmbedding(max_len, VOCAB_SIZE, d_model)\n","\n","# text의 각 단어에 512차원의 임베딩완료(임베딩 + 포지션)\n","txt = embedding_layer(txtInput)\n","\n","# 멀티헤드어텐션 + FFNN 네트워크, num_layers 만큼 존재\n","# 입력 : txt : (None, 40, 512)\n","# 출력 : txtOutput, (None, 40, 512)\n","txtOutput = encoder(VOCAB_SIZE, num_layers=num_layers, dff=dff, \n","                     d_model=d_model, num_heads=num_heads,\n","                     dropout=dropout)([txt, padding_mask])\n","\n","# 이미지 입력 후 CNN 통과\n","# 출력 : imgOutput, (None, 40, 512)\n","img = Conv2D(64, (3,3), padding='same', name='block1_conv1', activation='relu')(imgInput)\n","#img = Conv2D(64, (3,3), padding='same', name='block1_conv2', activation='relu')(img)\n","img = MaxPooling2D((4,4), name='block1_pool')(img)\n","img = Conv2D(128, (3,3), padding='same', name='block2_conv1', activation='relu')(img)\n","#img = Conv2D(128, (3,3), padding='same', name='block2_conv2', activation='relu')(img)\n","img = MaxPooling2D((4,4), name='block2_pool')(img)\n","img = Conv2D(256, (3,3), padding='same', name='block3_conv1', activation='relu')(img)\n","#img = Conv2D(256, (3,3), padding='same', name='block3_conv2', activation='relu')(img)\n","img = MaxPooling2D((4,4), name='block3_pool')(img)\n","#img = Conv2D(512, (3,3), padding='same', name='block4_conv1', activation='relu')(img)\n","#img = Conv2D(512, (3,3), padding='same', name='block4_conv2', activation='relu')(img)\n","#img = MaxPooling2D((2,2), name='block4_pool')(img)\n","#img = Conv2D(512, (3,3), padding='same', name='block5_conv1', activation='relu')(img)\n","#img = Conv2D(512, (3,3), padding='same', name='block5_conv2', activation='relu')(img)\n","#img = MaxPooling2D((2,2), name='block5_pool')(img)\n","img = Flatten()(img)\n","img = Dense(d_model, activation='relu')(img)\n","imgOutput = img\n","for i in range(max_len-1):\n","  imgOutput = concatenate([imgOutput, img])\n","imgOutput = Reshape((max_len,d_model))(imgOutput)\n","\n","imgAttention = txtimg_encoder(vocab_size=VOCAB_SIZE, num_layers = num_layers, dff=dff, d_model=d_model, num_heads=num_heads, \n","                     dropout=dropout)([txtOutput, imgOutput, padding_mask])\n","\n","outputs = imgtxt_encoder(vocab_size=VOCAB_SIZE, num_layers = num_layers, dff=dff, d_model=d_model, num_heads=num_heads,\n","                              dropout=dropout)([imgAttention, txtOutput, padding_mask])\n","outputs = Flatten()(outputs)\n","outputs = Dropout(dropout)(outputs)\n","outputs = Dense(d_model, activation='relu')(outputs)\n","outputs = Dropout(dropout)(outputs)\n","outputs = Dense(VOCAB_SIZE, activation='softmax')(outputs)\n","\n","\n","\n","model = Model(inputs = [txtInput, imgInput], outputs = outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"plYtBAKHLRXy"},"source":["learning_rate = CustomSchedule(d_model)\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1 = 0.9, beta_2 = 0.98, epsilon=1e-9)\n","\n","model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xe8L145LnJNb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624434724738,"user_tz":-540,"elapsed":10731697,"user":{"displayName":"오세정","photoUrl":"","userId":"10647477815584692687"}},"outputId":"0fc2dd61-451d-455a-e722-4b2077c9b349"},"source":["# text_data : 질문 데이터\n","# img_data : 이미지 데이터\n","# answers : 정답 데이터 (인코딩 된)\n","EPOCHS = 3\n","\n","history = model.fit(\n","    x = train_generator, \n","    epochs = EPOCHS,\n","    validation_data = valid_generator,\n","    workers = 6\n","    # callbacks = callback_list\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","6951/6951 [==============================] - 6970s 1s/step - loss: 3.0764 - accuracy: 0.5541 - val_loss: 1.1491 - val_accuracy: 0.5967\n","Epoch 2/3\n","6951/6951 [==============================] - 7361s 1s/step - loss: 1.1379 - accuracy: 0.5994 - val_loss: 1.0920 - val_accuracy: 0.6123\n","Epoch 3/3\n","6951/6951 [==============================] - 7430s 1s/step - loss: 1.0959 - accuracy: 0.6109 - val_loss: 1.0807 - val_accuracy: 0.6127\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"blkDQ3x1hDOo"},"source":["# 제출"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cRQCWp6ubxvd","executionInfo":{"status":"ok","timestamp":1624435759591,"user_tz":-540,"elapsed":839243,"user":{"displayName":"오세정","photoUrl":"","userId":"10647477815584692687"}},"outputId":"a5851ebd-ef34-4401-cc0d-56ece78da4db"},"source":["from google.colab import files\n","# 파일 이름 형식\n","# layer(트랜스포머 층 개수)_dmodel(임베딩벡터 크기)_dff(피드포워드신경망크기)_numhead(멀티 헤드 개수)\n","# _img(conv-maxpooling 층 개수)(한 층당 conv 층개수)_epoch(에포크횟수)_ETA(총 걸린시간)_valaccuracy(정확도)_testaccuracy(정확도)_(날짜)\n","model.save_weights('5_256_512_16_31_3_6_6107_6123_0623')\n","files.download('5_256_512_16_31_3_6_6107_6123_0623.index')\n","\n","# 기존 images 폴더를 test 안으로 바꾸고 실행\n","!mkdir test\n","!unzip \"./drive/My Drive/vqa/test.zip\" -d \"./test\"\n","\n","# 드라이브에 있는 test.csv를 업로드 하고 실행\n","test_data = pd.read_csv('./drive/MyDrive/vqa/test.csv')\n","test_generator = DataGenerator_test(test_data, tokenizer=tokenizer, vocab_size=VOCAB_SIZE, \n","                  batch_size=1,\n","                  target_size = (TARGET_SIZE, TARGET_SIZE),\n","                  shuffle=False)\n","\n","outputs = model.predict(x = test_generator, workers = 6, verbose = 1)\n","output = np.argmax(outputs, axis = 1)\n","test_answer = tokenizer.convert_ids_to_tokens(output)\n","data = {\n","    'ID':test_data['ID'],\n","    'answer':test_answer,\n","}\n","submission = pd.DataFrame(data)\n","submission.set_index('ID',inplace=True)\n","submission.to_csv('./submission.csv')\n","files.download('./submission.csv')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["22131/22131 [==============================] - 922s 41ms/step\n"],"name":"stdout"}]}]}